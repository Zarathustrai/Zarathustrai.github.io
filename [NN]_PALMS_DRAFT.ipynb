{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[NN] PALMS - DRAFT ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGi9KGyXl3EI9Tt2UfC78K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zarathustrai/Zarathustrai.github.io/blob/main/%5BNN%5D_PALMS_DRAFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PALMS\n",
        "### Prerequisites"
      ],
      "metadata": {
        "id": "kLvVXbx9Xxlt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi2_lSHvtwZ0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from PIL import Image\n",
        "from keras.models import Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "xFwVVdU9XFNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf12bc15-9e9e-473e-e156-16d0b1c7c4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "# make sure it is GPU:0\n",
        "print(device_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVs5OFBuAiP4",
        "outputId": "bc7c90f7-3ff1-432e-eba3-71ced33ca528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Dataset into Training and Validation set\n",
        "\n",
        "class config:\n",
        "    # specify path to the dataset\n",
        "    # ! ADJUST AS NEEDED\n",
        "    DATASET_PATH = 'drive/MyDrive/palm_data/Hands/Hands'\n",
        "    CSV_PATH = 'drive/MyDrive/palm_data/HandInfo.csv'\n",
        "    ROOT = os.getcwd()\n",
        "    # path for saving output; specify the paths to our training and validation set\n",
        "    OUTPUT = \"output\"\n",
        "    TRAIN = \"train\"\n",
        "    TEST = \"test\"\n",
        "    VAL = \"val\"\n",
        "\n",
        "    # set the crop size\n",
        "    INPUT_CROPSIZE = 224\n",
        "\n",
        "    # set the batch size and validation data split\n",
        "    BATCH_SIZE = 32\n",
        "    TEST_BATCH_SIZE = 64\n",
        "    # TEST_BATCH_SIZE = 8\n",
        "    # BATCH_SIZE = 8 ### for testing\n",
        "    TRAIN_SPLIT = 0.9\n",
        "\n",
        "    EPOCHS = 20\n",
        "    # EPOCHS = 2 ### for testing\n",
        "    LEARNING_RATE = 1e-3"
      ],
      "metadata": {
        "id": "0K5-4eEmxsJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "Y6WuQYLAXM15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_images(imagePaths, folder):\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "  for path in imagePaths:\n",
        "    shutil.copy(config.DATASET_PATH + '/' + path, folder)"
      ],
      "metadata": {
        "id": "Ip25vkRDu2gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(pd_series):\n",
        "    labels = pd_series.value_counts().index.tolist()\n",
        "    counts = pd_series.value_counts().values.tolist()\n",
        "    \n",
        "    pie_plot = go.Pie(labels=labels, values=counts, hole=.3)\n",
        "    fig = go.Figure(data=[pie_plot])\n",
        "    fig.update_layout(title_text='Distribution for %s' % pd_series.name)\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "plot_distribution(df.gender)\n",
        "plot_distribution(df.accessories)\n",
        "plot_distribution(df.nailPolish)\n",
        "plot_distribution(df.aspectOfHand)\n",
        "plot_distribution(df.skinColor)\n",
        "bins = [10, 20, 30, 40, 60, 80]\n",
        "names = ['10-20', '20-30', '30-40', '40-60', '60-80']\n",
        "age_binned = pd.cut(df['age'], bins, labels=names)\n",
        "plot_distribution(age_binned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "aSbOZbNyFcGl",
        "outputId": "87f30982-0c18-4bd1-c1f1-6c55b87195f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d3f055bbc86c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccessories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnailPolish\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE IMAGE PATHS\n",
        "def create_paths(image_names):\n",
        "    np.random.shuffle(image_names)\n",
        "    trainPathsLen = int(len(image_names) * config.TRAIN_SPLIT)\n",
        "    trainPaths = image_names[:trainPathsLen]\n",
        "    testPaths = image_names[trainPathsLen:]\n",
        "    trainPathsLen = int(trainPathsLen * config.TRAIN_SPLIT)\n",
        "    trainPaths, valPaths = trainPaths[:trainPathsLen], trainPaths[trainPathsLen:]\n",
        "    return trainPaths, valPaths, testPaths"
      ],
      "metadata": {
        "id": "Vkj2ULIRlMux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert aliases into ID\n",
        "def alias_to_ID(dataset):\n",
        "    dataset_dict = {\n",
        "        'skin_id': {\n",
        "            0: 'very fair',\n",
        "            1: 'fair',\n",
        "            2: 'medium',\n",
        "            3: 'dark'\n",
        "        },\n",
        "        'gender_id': {\n",
        "            0: 'male',\n",
        "            1: 'female'\n",
        "        },\n",
        "        'palm_id': {\n",
        "            0: 'palmar right',\n",
        "            1: 'dorsal right',\n",
        "            2: 'palmar left',\n",
        "            3: 'dorsal left'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
        "    dataset_dict['skin_alias'] = dict((s, i) for i, s in dataset_dict['skin_id'].items())\n",
        "    dataset_dict['palm_alias'] = dict((p, i) for i, p in dataset_dict['palm_id'].items())\n",
        "\n",
        "    dataset['gender_id'] = dataset['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n",
        "    dataset['skin_id'] = dataset['skinColor'].map(lambda skinColor: dataset_dict['skin_alias'][skinColor])\n",
        "    dataset['palm_id'] = dataset['aspectOfHand'].map(lambda aspectOfHand: dataset_dict['palm_alias'][aspectOfHand])\n",
        "    max_age = dataset['age'].max()\n",
        "    return df, max_age, dataset_dict\n"
      ],
      "metadata": {
        "id": "3O6Hyez3P6vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "def preprocess_image(img_path):\n",
        "    im = Image.open(img_path)\n",
        "    im = im.resize((config.INPUT_CROPSIZE, config.INPUT_CROPSIZE))\n",
        "    im = np.array(im) / 255.0\n",
        "    im = tf.image.random_flip_left_right(im)\n",
        "    im = tf.image.random_flip_up_down(im)\n",
        "    im = tf.keras.preprocessing.image.random_rotation(im, 15)\n",
        "    return im"
      ],
      "metadata": {
        "id": "-oBThErbQUiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE BATCH BASED ON IMAGENAME\n",
        "def generate_images(img_paths, batch_size, is_training):\n",
        "    # arrays to store our batched data\n",
        "    images, ages, skins, genders, accessories, palms, nails = [], [], [], [], [], [], []\n",
        "    show = True\n",
        "    while True:\n",
        "        for path in img_paths:\n",
        "            # find person row based on imageName\n",
        "            person = df.iloc[df.index[df.imageName == path]]\n",
        "            age = person['age']\n",
        "            skin = person['skin_id']\n",
        "            gender = person['gender_id']\n",
        "            accessory = person['accessories']\n",
        "            palm = person['palm_id']\n",
        "            nail = person['nailPolish']\n",
        "            im = preprocess_image(config.DATASET_PATH + '/' + path)  # full path\n",
        "            # if show:\n",
        "            #     plt.imshow(im, interpolation='nearest')\n",
        "            #     plt.show()\n",
        "            #     show = False\n",
        "\n",
        "            ages.append(age / max_age)\n",
        "            skins.append(skin)\n",
        "            genders.append(gender)\n",
        "            accessories.append(accessory)\n",
        "            palms.append(palm)\n",
        "            nails.append(nail)\n",
        "            images.append(im)\n",
        "\n",
        "            # yielding condition\n",
        "            if len(images) >= batch_size:\n",
        "                yield np.array(images), [np.array(ages), np.array(skins), np.array(genders), np.array(accessories),\n",
        "                                         np.array(palms), np.array(nails)]\n",
        "                images, ages, skins, genders, accessories, palms, nails = [], [], [], [], [], [], []\n",
        "        if not is_training:\n",
        "            break"
      ],
      "metadata": {
        "id": "AxRT83zxklGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING RESULTS\n",
        "def evaluate(testPaths, model, max_age):\n",
        "    test_generator = generate_images(testPaths, is_training=False, batch_size=config.TEST_BATCH_SIZE)\n",
        "    age_pred, gender_pred, race_pred, palm_pred, accessories_pred, nail_pred = model.predict_generator(test_generator,\n",
        "                                                               steps=len(testPaths) // config.TEST_BATCH_SIZE)\n",
        "    test_generator = generate_images(testPaths, is_training=False, batch_size=config.TEST_BATCH_SIZE)\n",
        "\n",
        "    images, age_true, gender_true, race_true, palm_true, accessories_true, nail_true = [], [], [], [], [], [], []\n",
        "    for test_batch in test_generator:\n",
        "        image = test_batch[0]\n",
        "        labels = test_batch[1]\n",
        "\n",
        "        images.extend(image)\n",
        "        age_true.extend(labels[0])\n",
        "        gender_true.extend(labels[1])\n",
        "        race_true.extend(labels[2])\n",
        "        palm_true.extend(labels[3])\n",
        "        accessories_true.extend(labels[4])\n",
        "        nail_true.extend(labels[5])\n",
        "\n",
        "    age_true = np.array(age_true)\n",
        "    gender_true = np.array(gender_true)\n",
        "    race_true = np.array(race_true)\n",
        "    palm_true = np.array(palm_true)\n",
        "    accessories_true = np.array(accessories_true)\n",
        "    nail_true = np.array(nail_true)\n",
        "\n",
        "    race_true, gender_true, palm_true, accessories_true, nail_true = race_true.argmax(axis=-1), \\\n",
        "                                                                     gender_true.argmax(axis=-1), \\\n",
        "                                                                     palm_true.argmax(axis=-1), \\\n",
        "                                                                     accessories_true.argmax(axis=-1), \\\n",
        "                                                                     nail_true.argmax(axis=-1)\n",
        "    gender_pred, race_pred, palm_pred, accessories_pred, nail_pred  = gender_pred.argmax(axis=-1), \\\n",
        "                                                                      race_pred.argmax(axis=-1), \\\n",
        "                                                                      palm_pred.argmax(axis=-1), \\\n",
        "                                                                      accessories_pred.argmax(axis=-1), \\\n",
        "                                                                      nail_pred.argmax(axis=-1)\n",
        "    age_true = age_true * max_age\n",
        "    age_pred = age_pred * max_age\n",
        "    return age_true, race_true, gender_true, palm_true, accessories_true, nail_true, age_pred, gender_pred, race_pred, palm_pred, accessories_pred, nail_pred"
      ],
      "metadata": {
        "id": "Bb5E3HC8lYg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INFER STATS FROM TESTING + EXPORT TO TXT\n",
        "def statistics(dataset_dict, age_true, race_true, gender_true, palm_true, accessories_true, nail_true, age_pred, gender_pred, race_pred, palm_pred, accessories_pred, nail_pred):\n",
        "\n",
        "    if not os.path.exists(\"statistics\"):\n",
        "        os.mkdir(\"statistics\")\n",
        "\n",
        "    cr_gender = classification_report(gender_true, gender_pred)\n",
        "    cr_skin = classification_report(race_true, race_pred)\n",
        "    cr_palm = classification_report(palm_true, palm_pred)\n",
        "    cr_accessories = classification_report(accessories_true, accessories_pred)\n",
        "    cr_nail = classification_report(nail_true, nail_pred)\n",
        "\n",
        "    gender = open(\"statistics/genderStat.txt\", \"w\")\n",
        "    gender.write(cr_gender)\n",
        "    gender.close()\n",
        "\n",
        "    skin = open(\"statistics/skinStat.txt\", \"w\")\n",
        "    skin.write(cr_skin)\n",
        "    skin.close()\n",
        "\n",
        "    palm = open(\"statistics/palmStat.txt\", \"w\")\n",
        "    palm.write(cr_palm)\n",
        "    palm.close()\n",
        "\n",
        "    accessories = open(\"statistics/accessoriesStat.txt\", \"w\")\n",
        "    accessories.write(cr_accessories)\n",
        "    accessories.close()\n",
        "\n",
        "    nail = open(\"statistics/nailStat.txt\", \"w\")\n",
        "    nail.write(cr_nail)\n",
        "    nail.close()\n",
        "\n",
        "    age = open(\"statistics/ageR2.txt\", \"w\")\n",
        "    age.write(str(r2_score(age_true, age_pred)))\n",
        "    age.close()"
      ],
      "metadata": {
        "id": "p-CJ4iazlau5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN / VALUATION RESULTS TO PNG GRAPHS\n",
        "def plot_results(history):\n",
        "\n",
        "    if not os.path.exists(\"images\"):\n",
        "        os.mkdir(\"images\")\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scattergl(\n",
        "        y=history.history['loss'],\n",
        "        name='Train'))\n",
        "    fig.add_trace(go.Scattergl(\n",
        "        y=history.history['val_loss'],\n",
        "        name='Valid'))\n",
        "    fig.update_layout(height=500,\n",
        "                      width=700,\n",
        "                      title='Overall loss',\n",
        "                      xaxis_title='Epoch',\n",
        "                      yaxis_title='Loss')\n",
        "    fig.show()\n",
        "    fig.write_image(\"images/trainValLoss.png\")\n",
        "\n",
        "    plt.clf()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['race_output_accuracy'],\n",
        "        name='Train'))\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_race_output_accuracy'],\n",
        "        name='Valid'))\n",
        "    fig.update_layout(height=500,\n",
        "                      width=700,\n",
        "                      title='Accuracy for race feature',\n",
        "                      xaxis_title='Epoch',\n",
        "                      yaxis_title='Accuracy')\n",
        "    fig.show()\n",
        "    fig.write_image(\"images/VTRaceOutputAccuracy.png\")\n",
        "\n",
        "    plt.clf()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['gender_output_accuracy'],\n",
        "        name='Train'))\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_gender_output_accuracy'],\n",
        "        name='Valid'))\n",
        "    fig.update_layout(height=500,\n",
        "                      width=700,\n",
        "                      title='Accuracy for gender feature',\n",
        "                      xaxis_title='Epoch',\n",
        "                      yaxis_title='Accuracy')\n",
        "    fig.show()\n",
        "    fig.write_image(\"images/VTGenderOutputAccuracy.png\")\n",
        "\n",
        "    plt.clf()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['accessories_output_accuracy'],\n",
        "        name='Train'))\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_accessories_output_accuracy'],\n",
        "        name='Valid'))\n",
        "    fig.update_layout(height=500,\n",
        "                      width=700,\n",
        "                      title='Accuracy for accessories feature',\n",
        "                      xaxis_title='Epoch',\n",
        "                      yaxis_title='Accuracy')\n",
        "    fig.show()\n",
        "    fig.write_image(\"images/VTAccessoriesOutputAccuracy.png\")\n",
        "\n",
        "    plt.clf()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['palm_output_accuracy'],\n",
        "        name='Train'))\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_palm_output_accuracy'],\n",
        "        name='Valid'))\n",
        "    fig.update_layout(height=500,\n",
        "                      width=700,\n",
        "                      title='Accuracy for palm feature',\n",
        "                      xaxis_title='Epoch',\n",
        "                      yaxis_title='Accuracy')\n",
        "    fig.show()\n",
        "    fig.write_image(\"images/VTPalmOutputAccuracy.png\")\n",
        "\n",
        "    plt.clf()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scattergl(\n",
        "        y=history.history['age_output_mae'],\n",
        "        name='Train'))\n",
        "    fig.add_trace(go.Scattergl(\n",
        "        y=history.history['val_age_output_mae'],\n",
        "        name='Valid'))\n",
        "    fig.update_layout(height=500,\n",
        "                      width=700,\n",
        "                      title='Mean Absolute Error for age feature',\n",
        "                      xaxis_title='Epoch',\n",
        "                      yaxis_title='Mean Absolute Error')\n",
        "    fig.show()\n",
        "    fig.write_image(\"images/VTAccessoriesOutputMAE.png\")"
      ],
      "metadata": {
        "id": "OUrEBHbpld13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN / VALUATION RESULTS TO CSV\n",
        "def train_val_results(history):\n",
        "\n",
        "    if not os.path.exists(\"trainValRes\"):\n",
        "        os.mkdir(\"trainValRes\")\n",
        "\n",
        "    a = np.array(history.history['age_output_mae'])\n",
        "    b = np.array(history.history['race_output_accuracy'])\n",
        "    c = np.array(history.history['gender_output_accuracy'])\n",
        "    d = np.array(history.history['nail_output_accuracy'])\n",
        "    e = np.array(history.history['accessories_output_accuracy'])\n",
        "    f = np.array(history.history['palm_output_accuracy'])\n",
        "\n",
        "    loss = np.array(history.history['loss'])\n",
        "    val_loss = np.array(history.history['val_loss'])\n",
        "\n",
        "    results = pd.DataFrame({\"age_mae\": a, \"race_acc\": b, \"gender_acc\": c, \"nail_acc\": d, \"accessories_acc\": e, \"palm_acc\": f})\n",
        "    results.to_csv(\"trainValRes/output.csv\", index = False)\n",
        "\n",
        "    a = np.array(history.history['val_age_output_mae'])\n",
        "    b = np.array(history.history['val_race_output_accuracy'])\n",
        "    c = np.array(history.history['val_gender_output_accuracy'])\n",
        "    d = np.array(history.history['val_nail_output_accuracy'])\n",
        "    e = np.array(history.history['val_accessories_output_accuracy'])\n",
        "    f = np.array(history.history['val_palm_output_accuracy'])\n",
        "    val_results = pd.DataFrame({\"val_age_mae\": a, \"val_race_acc\": b, \"val_gender_acc\": c, \"val_nail_acc\": d, \"val_accessories_acc\": e, \"val_palm_acc\": f})\n",
        "    val_results.to_csv(\"trainValRes/val_output.csv\", index = False)\n",
        "\n",
        "    loss_results = pd.DataFrame({\"loss\": loss, \"val_loss\": val_loss})\n",
        "    loss_results.to_csv(\"trainValRes/loss_output.csv\", index = False)"
      ],
      "metadata": {
        "id": "EJv692gAlho5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD MODEL\n",
        "class MultiModel():\n",
        "    def make_default_hidden_layers(self, inputs):\n",
        "        \"\"\"\n",
        "        Used to generate a default set of hidden layers. The structure used in this network is defined as:\n",
        "        Conv2D -> BatchNormalization -> Pooling -> Dropout\n",
        "        \"\"\"\n",
        "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "\n",
        "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "        return x\n",
        "\n",
        "    def build_accessories_branch(self, inputs, num_accessories =2):\n",
        "        \"\"\"\n",
        "        Used to build the accessories branch of our palm recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_accessories)(x)\n",
        "        x = Activation(\"sigmoid\", name=\"accessories_output\")(x)\n",
        "        return x\n",
        "\n",
        "    def build_palm_branch(self, inputs, num_palm = 4):\n",
        "        \"\"\"\n",
        "        Used to build the palm branch of our palm recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_palm)(x)\n",
        "        x = Activation(\"softmax\", name=\"palm_output\")(x)\n",
        "        return x    \n",
        "\n",
        "    def build_race_branch(self, inputs, num_skinColor = 4):\n",
        "        \"\"\"\n",
        "        Used to build the skin color branch of our palm recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_skinColor)(x)\n",
        "        x = Activation(\"softmax\", name=\"race_output\")(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def build_gender_branch(self, inputs, num_genders=2):\n",
        "        \"\"\"\n",
        "        Used to build the gender branch of our palm recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_genders)(x)\n",
        "        x = Activation(\"sigmoid\", name=\"gender_output\")(x)\n",
        "        return x\n",
        "\n",
        "    def build_nail_branch(self, inputs, num_nails = 2):\n",
        "        \"\"\"\n",
        "        Used to build the nail branch of our palm recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(num_nails)(x)\n",
        "        x = Activation(\"sigmoid\", name=\"nail_output\")(x)\n",
        "        return x\n",
        "\n",
        "    def build_age_branch(self, inputs):   \n",
        "        \"\"\"\n",
        "        Used to build the age branch of our palm recognition network.\n",
        "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
        "        followed by the Dense output layer.\n",
        "        \"\"\"\n",
        "        x = self.make_default_hidden_layers(inputs)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(1)(x)\n",
        "        x = Activation(\"linear\", name=\"age_output\")(x)\n",
        "        return x\n",
        "\n",
        "    def assemble_full_model(self, width, height):\n",
        "        \"\"\"\n",
        "        Used to assemble our multi-output model CNN.\n",
        "        \"\"\"\n",
        "        input_shape = (height, width, 3)\n",
        "        inputs = Input(shape=input_shape)\n",
        "        age_branch = self.build_age_branch(inputs)\n",
        "        race_branch = self.build_race_branch(inputs)\n",
        "        gender_branch = self.build_gender_branch(inputs)\n",
        "        palm_branch = self.build_palm_branch(inputs)\n",
        "        accessories_branch = self.build_accessories_branch(inputs)\n",
        "        nail_branch = self.build_nail_branch(inputs)\n",
        "\n",
        "        model = Model(inputs=inputs,\n",
        "                     outputs = [age_branch, gender_branch, race_branch, palm_branch, accessories_branch, nail_branch],\n",
        "                     name=\"palm_net\")\n",
        "        return model\n",
        "    \n",
        "model = MultiModel().assemble_full_model(width = config.INPUT_CROPSIZE, height = config.INPUT_CROPSIZE)\n"
      ],
      "metadata": {
        "id": "3CcHYNBIXURI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPILE + TRAIN/TEST MODEL\n",
        "df = pd.read_csv(config.CSV_PATH)\n",
        "image_names = df.imageName\n",
        "trainPaths, valPaths, testPaths = create_paths(image_names)\n",
        "copy_images(trainPaths, config.TRAIN)\n",
        "copy_images(valPaths, config.VAL)\n",
        "copy_images(testPaths, config.TEST)\n",
        "\n",
        "df, max_age, dataset_dict = alias_to_ID(df)\n",
        "\n",
        "model = MultiModel().assemble_full_model(width=config.INPUT_CROPSIZE, height=config.INPUT_CROPSIZE)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE, decay=config.LEARNING_RATE / config.EPOCHS)\n",
        "model.compile(optimizer=opt,\n",
        "              loss={\n",
        "                  'age_output': 'mse',\n",
        "                  'race_output': 'categorical_crossentropy',\n",
        "                  'gender_output': 'binary_crossentropy',\n",
        "                  'nail_output': 'binary_crossentropy',\n",
        "                  'accessories_output': 'binary_crossentropy',\n",
        "                  'palm_output': 'categorical_crossentropy'},\n",
        "              loss_weights={\n",
        "                  'age_output': 4.,\n",
        "                  'race_output': 1.2,\n",
        "                  'gender_output': 0.1,\n",
        "                  'nail_output': 0.1,\n",
        "                  'accessories_output': 0.1,\n",
        "                  'palm_output': 1.2},\n",
        "              metrics={\n",
        "                  'age_output': 'mae',\n",
        "                  'race_output': 'accuracy',\n",
        "                  'gender_output': 'accuracy',\n",
        "                  'nail_output': 'accuracy',\n",
        "                  'accessories_output': 'accuracy',\n",
        "                  'palm_output': 'accuracy'})\n",
        "\n",
        "train_gen = generate_images(trainPaths, batch_size=config.BATCH_SIZE, is_training = True)\n",
        "valid_gen = generate_images(valPaths, batch_size=config.BATCH_SIZE, is_training= True)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
        "]\n",
        "\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=len(trainPaths) // config.BATCH_SIZE,\n",
        "                    epochs=config.EPOCHS,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=valid_gen,\n",
        "                    validation_steps=len(valPaths) // config.BATCH_SIZE\n",
        "                    )\n",
        "\n",
        "train_val_results(history)\n",
        "plot_results(history)\n",
        "age_true, race_true, gender_true, palm_true, accessories_true, nail_true, age_pred, gender_pred, race_pred, palm_pred, accessories_pred, nail_pred = evaluate(testPaths, model, max_age)\n",
        "statistics(dataset_dict, age_true, race_true, gender_true, palm_true, accessories_true, nail_true, age_pred, gender_pred, race_pred, palm_pred, accessories_pred, nail_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osdU-o_wcc70",
        "outputId": "885052e3-bd4f-451b-e148-6d6ddc514676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: you are shuffling a 'Series' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "280/280 [==============================] - ETA: 0s - loss: 12.4164 - age_output_loss: 1.0615 - gender_output_loss: -1.7058 - race_output_loss: 4.9602 - palm_output_loss: 1.9840 - accessories_output_loss: -0.4597 - nail_output_loss: 0.5381 - age_output_mae: 0.7308 - gender_output_accuracy: 0.4815 - race_output_accuracy: 0.2555 - palm_output_accuracy: 0.2579 - accessories_output_accuracy: 0.4794 - nail_output_accuracy: 0.4653INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1445s 5s/step - loss: 12.4164 - age_output_loss: 1.0615 - gender_output_loss: -1.7058 - race_output_loss: 4.9602 - palm_output_loss: 1.9840 - accessories_output_loss: -0.4597 - nail_output_loss: 0.5381 - age_output_mae: 0.7308 - gender_output_accuracy: 0.4815 - race_output_accuracy: 0.2555 - palm_output_accuracy: 0.2579 - accessories_output_accuracy: 0.4794 - nail_output_accuracy: 0.4653 - val_loss: 1703.4702 - val_age_output_loss: 420.2975 - val_gender_output_loss: -79.5996 - val_race_output_loss: 9.9749 - val_palm_output_loss: 16.3552 - val_accessories_output_loss: -13.5607 - val_nail_output_loss: 0.0028 - val_age_output_mae: 19.2558 - val_gender_output_accuracy: 1.0000 - val_race_output_accuracy: 0.0010 - val_palm_output_accuracy: 0.0000e+00 - val_accessories_output_accuracy: 0.0030 - val_nail_output_accuracy: 0.0575\n",
            "Epoch 2/20\n",
            "280/280 [==============================] - ETA: 0s - loss: 6.5502 - age_output_loss: 0.1776 - gender_output_loss: -16.0031 - race_output_loss: 4.7807 - palm_output_loss: 1.8022 - accessories_output_loss: -4.9764 - nail_output_loss: 0.3843 - age_output_mae: 0.3018 - gender_output_accuracy: 0.4002 - race_output_accuracy: 0.2479 - palm_output_accuracy: 0.2459 - accessories_output_accuracy: 0.4732 - nail_output_accuracy: 0.4741INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1412s 5s/step - loss: 6.5502 - age_output_loss: 0.1776 - gender_output_loss: -16.0031 - race_output_loss: 4.7807 - palm_output_loss: 1.8022 - accessories_output_loss: -4.9764 - nail_output_loss: 0.3843 - age_output_mae: 0.3018 - gender_output_accuracy: 0.4002 - race_output_accuracy: 0.2479 - palm_output_accuracy: 0.2459 - accessories_output_accuracy: 0.4732 - nail_output_accuracy: 0.4741 - val_loss: 5020.8682 - val_age_output_loss: 1255.1906 - val_gender_output_loss: -28.3627 - val_race_output_loss: 2.6624 - val_palm_output_loss: 1.0928 - val_accessories_output_loss: -15.7441 - val_nail_output_loss: 0.1016 - val_age_output_mae: 27.0544 - val_gender_output_accuracy: 0.0978 - val_race_output_accuracy: 0.5625 - val_palm_output_accuracy: 0.0141 - val_accessories_output_accuracy: 1.0000 - val_nail_output_accuracy: 0.0312\n",
            "Epoch 3/20\n",
            "280/280 [==============================] - ETA: 0s - loss: 2.2787 - age_output_loss: 0.0390 - gender_output_loss: -50.9106 - race_output_loss: 5.3910 - palm_output_loss: 1.8163 - accessories_output_loss: -14.7277 - nail_output_loss: 0.3785 - age_output_mae: 0.1363 - gender_output_accuracy: 0.3616 - race_output_accuracy: 0.2559 - palm_output_accuracy: 0.2635 - accessories_output_accuracy: 0.4647 - nail_output_accuracy: 0.4773INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1400s 5s/step - loss: 2.2787 - age_output_loss: 0.0390 - gender_output_loss: -50.9106 - race_output_loss: 5.3910 - palm_output_loss: 1.8163 - accessories_output_loss: -14.7277 - nail_output_loss: 0.3785 - age_output_mae: 0.1363 - gender_output_accuracy: 0.3616 - race_output_accuracy: 0.2559 - palm_output_accuracy: 0.2635 - accessories_output_accuracy: 0.4647 - nail_output_accuracy: 0.4773 - val_loss: 2844.5139 - val_age_output_loss: 712.1176 - val_gender_output_loss: -121.7575 - val_race_output_loss: 2.2226 - val_palm_output_loss: 1.4886 - val_accessories_output_loss: 37.5184 - val_nail_output_loss: 0.1429 - val_age_output_mae: 17.8621 - val_gender_output_accuracy: 1.0000 - val_race_output_accuracy: 0.4415 - val_palm_output_accuracy: 0.0948 - val_accessories_output_accuracy: 0.9798 - val_nail_output_accuracy: 0.0060\n",
            "Epoch 4/20\n",
            "280/280 [==============================] - ETA: 0s - loss: -4.7140 - age_output_loss: 0.0149 - gender_output_loss: -104.9797 - race_output_loss: 5.5793 - palm_output_loss: 1.7600 - accessories_output_loss: -31.2000 - nail_output_loss: 0.3699 - age_output_mae: 0.0681 - gender_output_accuracy: 0.3353 - race_output_accuracy: 0.2520 - palm_output_accuracy: 0.2541 - accessories_output_accuracy: 0.4930 - nail_output_accuracy: 0.4891INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1405s 5s/step - loss: -4.7140 - age_output_loss: 0.0149 - gender_output_loss: -104.9797 - race_output_loss: 5.5793 - palm_output_loss: 1.7600 - accessories_output_loss: -31.2000 - nail_output_loss: 0.3699 - age_output_mae: 0.0681 - gender_output_accuracy: 0.3353 - race_output_accuracy: 0.2520 - palm_output_accuracy: 0.2541 - accessories_output_accuracy: 0.4930 - nail_output_accuracy: 0.4891 - val_loss: 2532.9255 - val_age_output_loss: 635.6180 - val_gender_output_loss: -107.6278 - val_race_output_loss: 4.4536 - val_palm_output_loss: 1.0665 - val_accessories_output_loss: -54.2298 - val_nail_output_loss: 0.1494 - val_age_output_mae: 16.5260 - val_gender_output_accuracy: 0.0343 - val_race_output_accuracy: 0.0050 - val_palm_output_accuracy: 0.2298 - val_accessories_output_accuracy: 0.5605 - val_nail_output_accuracy: 0.0323\n",
            "Epoch 5/20\n",
            "280/280 [==============================] - ETA: 0s - loss: -13.8722 - age_output_loss: 0.0120 - gender_output_loss: -175.8328 - race_output_loss: 5.7445 - palm_output_loss: 1.7370 - accessories_output_loss: -53.5125 - nail_output_loss: 0.3668 - age_output_mae: 0.0526 - gender_output_accuracy: 0.3158 - race_output_accuracy: 0.2548 - palm_output_accuracy: 0.2326 - accessories_output_accuracy: 0.4760 - nail_output_accuracy: 0.5030INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1415s 5s/step - loss: -13.8722 - age_output_loss: 0.0120 - gender_output_loss: -175.8328 - race_output_loss: 5.7445 - palm_output_loss: 1.7370 - accessories_output_loss: -53.5125 - nail_output_loss: 0.3668 - age_output_mae: 0.0526 - gender_output_accuracy: 0.3158 - race_output_accuracy: 0.2548 - palm_output_accuracy: 0.2326 - accessories_output_accuracy: 0.4760 - nail_output_accuracy: 0.5030 - val_loss: 2366.9685 - val_age_output_loss: 597.0101 - val_gender_output_loss: -171.2256 - val_race_output_loss: 2.1130 - val_palm_output_loss: 1.0370 - val_accessories_output_loss: -77.4146 - val_nail_output_loss: 0.1211 - val_age_output_mae: 15.8389 - val_gender_output_accuracy: 0.1542 - val_race_output_accuracy: 0.2077 - val_palm_output_accuracy: 0.1815 - val_accessories_output_accuracy: 0.5474 - val_nail_output_accuracy: 0.0040\n",
            "Epoch 6/20\n",
            "280/280 [==============================] - ETA: 0s - loss: -24.9010 - age_output_loss: 0.0119 - gender_output_loss: -260.2788 - race_output_loss: 5.8663 - palm_output_loss: 1.7269 - accessories_output_loss: -80.6899 - nail_output_loss: 0.3664 - age_output_mae: 0.0504 - gender_output_accuracy: 0.2997 - race_output_accuracy: 0.2587 - palm_output_accuracy: 0.2329 - accessories_output_accuracy: 0.4825 - nail_output_accuracy: 0.5044INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1418s 5s/step - loss: -24.9010 - age_output_loss: 0.0119 - gender_output_loss: -260.2788 - race_output_loss: 5.8663 - palm_output_loss: 1.7269 - accessories_output_loss: -80.6899 - nail_output_loss: 0.3664 - age_output_mae: 0.0504 - gender_output_accuracy: 0.2997 - race_output_accuracy: 0.2587 - palm_output_accuracy: 0.2329 - accessories_output_accuracy: 0.4825 - nail_output_accuracy: 0.5044 - val_loss: 1953.1718 - val_age_output_loss: 496.2442 - val_gender_output_loss: -254.6398 - val_race_output_loss: 3.1186 - val_palm_output_loss: 1.0624 - val_accessories_output_loss: -113.7200 - val_nail_output_loss: 0.1406 - val_age_output_mae: 14.3211 - val_gender_output_accuracy: 0.0464 - val_race_output_accuracy: 0.0000e+00 - val_palm_output_accuracy: 0.5282 - val_accessories_output_accuracy: 0.1663 - val_nail_output_accuracy: 0.4587\n",
            "Epoch 7/20\n",
            "280/280 [==============================] - ETA: 0s - loss: -37.5684 - age_output_loss: 0.0123 - gender_output_loss: -356.0731 - race_output_loss: 5.9591 - palm_output_loss: 1.7245 - accessories_output_loss: -112.6702 - nail_output_loss: 0.3635 - age_output_mae: 0.0504 - gender_output_accuracy: 0.2967 - race_output_accuracy: 0.2527 - palm_output_accuracy: 0.2346 - accessories_output_accuracy: 0.4690 - nail_output_accuracy: 0.5084INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1419s 5s/step - loss: -37.5684 - age_output_loss: 0.0123 - gender_output_loss: -356.0731 - race_output_loss: 5.9591 - palm_output_loss: 1.7245 - accessories_output_loss: -112.6702 - nail_output_loss: 0.3635 - age_output_mae: 0.0504 - gender_output_accuracy: 0.2967 - race_output_accuracy: 0.2527 - palm_output_accuracy: 0.2346 - accessories_output_accuracy: 0.4690 - nail_output_accuracy: 0.5084 - val_loss: 1025.1021 - val_age_output_loss: 269.2194 - val_gender_output_loss: -418.2120 - val_race_output_loss: 2.6104 - val_palm_output_loss: 1.0343 - val_accessories_output_loss: -143.4136 - val_nail_output_loss: 0.1350 - val_age_output_mae: 10.2482 - val_gender_output_accuracy: 1.0000 - val_race_output_accuracy: 0.6542 - val_palm_output_accuracy: 0.3236 - val_accessories_output_accuracy: 0.5514 - val_nail_output_accuracy: 0.0423\n",
            "Epoch 8/20\n",
            "280/280 [==============================] - ETA: 0s - loss: -51.8543 - age_output_loss: 0.0115 - gender_output_loss: -463.1965 - race_output_loss: 5.9131 - palm_output_loss: 1.7236 - accessories_output_loss: -147.8130 - nail_output_loss: 0.3654 - age_output_mae: 0.0481 - gender_output_accuracy: 0.3036 - race_output_accuracy: 0.2502 - palm_output_accuracy: 0.2692 - accessories_output_accuracy: 0.4550 - nail_output_accuracy: 0.5220INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1420s 5s/step - loss: -51.8543 - age_output_loss: 0.0115 - gender_output_loss: -463.1965 - race_output_loss: 5.9131 - palm_output_loss: 1.7236 - accessories_output_loss: -147.8130 - nail_output_loss: 0.3654 - age_output_mae: 0.0481 - gender_output_accuracy: 0.3036 - race_output_accuracy: 0.2502 - palm_output_accuracy: 0.2692 - accessories_output_accuracy: 0.4550 - nail_output_accuracy: 0.5220 - val_loss: 843.9037 - val_age_output_loss: 223.9635 - val_gender_output_loss: -411.8165 - val_race_output_loss: 5.3295 - val_palm_output_loss: 1.0623 - val_accessories_output_loss: -184.5313 - val_nail_output_loss: 0.1417 - val_age_output_mae: 9.5537 - val_gender_output_accuracy: 0.1089 - val_race_output_accuracy: 1.0000 - val_palm_output_accuracy: 0.5050 - val_accessories_output_accuracy: 0.6058 - val_nail_output_accuracy: 0.0927\n",
            "Epoch 9/20\n",
            "280/280 [==============================] - ETA: 0s - loss: -67.6136 - age_output_loss: 0.0117 - gender_output_loss: -579.3364 - race_output_loss: 5.9008 - palm_output_loss: 1.7232 - accessories_output_loss: -189.1190 - nail_output_loss: 0.3639 - age_output_mae: 0.0476 - gender_output_accuracy: 0.2965 - race_output_accuracy: 0.2547 - palm_output_accuracy: 0.2690 - accessories_output_accuracy: 0.4567 - nail_output_accuracy: 0.5021INFO:tensorflow:Assets written to: ./model_checkpoint/assets\n",
            "280/280 [==============================] - 1415s 5s/step - loss: -67.6136 - age_output_loss: 0.0117 - gender_output_loss: -579.3364 - race_output_loss: 5.9008 - palm_output_loss: 1.7232 - accessories_output_loss: -189.1190 - nail_output_loss: 0.3639 - age_output_mae: 0.0476 - gender_output_accuracy: 0.2965 - race_output_accuracy: 0.2547 - palm_output_accuracy: 0.2690 - accessories_output_accuracy: 0.4567 - nail_output_accuracy: 0.5021 - val_loss: 846.2116 - val_age_output_loss: 228.8974 - val_gender_output_loss: -498.5919 - val_race_output_loss: 2.0931 - val_palm_output_loss: 1.0671 - val_accessories_output_loss: -233.2674 - val_nail_output_loss: 0.1577 - val_age_output_mae: 9.4886 - val_gender_output_accuracy: 0.4587 - val_race_output_accuracy: 0.2722 - val_palm_output_accuracy: 0.1653 - val_accessories_output_accuracy: 0.3548 - val_nail_output_accuracy: 0.5635\n",
            "Epoch 10/20\n",
            " 57/280 [=====>........................] - ETA: 16:50 - loss: -83.8105 - age_output_loss: 0.0270 - gender_output_loss: -722.9651 - race_output_loss: 6.1230 - palm_output_loss: 1.2436 - accessories_output_loss: -204.8823 - nail_output_loss: 0.2638 - age_output_mae: 0.1002 - gender_output_accuracy: 0.2933 - race_output_accuracy: 0.2588 - palm_output_accuracy: 0.2834 - accessories_output_accuracy: 0.4578 - nail_output_accuracy: 0.4962"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history['race_output_accuracy'], history.history['gender_output_accuracy'], history.history['palm_output_accuracy'], history.history['accessories_output_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw7WNMROnY5R",
        "outputId": "7a205654-d94a-40ed-fa96-4a7cf97164f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3068181872367859, 0.25, 0.25, 0.20454545319080353, 0.21590909361839294] [0.5340909361839294, 0.46590909361839294, 0.5568181872367859, 0.5568181872367859, 0.39772728085517883] [0.27272728085517883, 0.27272728085517883, 0.28409090638160706, 0.17045454680919647, 0.20454545319080353] [0.5113636255264282, 0.4886363744735718, 0.5227272510528564, 0.40909090638160706, 0.5113636255264282]\n"
          ]
        }
      ]
    }
  ]
}